{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU/S/fTHPRfeHV7x3mj38H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawanacharya1979/ART-CoreEngine/blob/main/Lab3_Code(CS_599).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtOq8yN1prfx",
        "outputId": "15ec513f-018f-4a42-8d80-ab141f082cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training model with custom Batch Normalization:\n",
            "Epoch 1 (Batch Norm): Loss = 0.4602\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. Custom Normalization Functions\n",
        "# -----------------------------------------------------------\n",
        "def custom_batch_norm(x, gamma, beta, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Custom Batch Normalization.\n",
        "    Computes the mean and variance over the mini-batch (and spatial dimensions for CNNs)\n",
        "    then normalizes and rescales the input.\n",
        "\n",
        "    x: Input tensor.\n",
        "    gamma: Learnable scale parameter.\n",
        "    beta: Learnable shift parameter.\n",
        "    epsilon: Small constant for numerical stability.\n",
        "    \"\"\"\n",
        "    axes = list(range(len(x.shape) - 1))  # Normalize across batch, height, and width dimensions\n",
        "    batch_mean = tf.reduce_mean(x, axis=axes, keepdims=True)\n",
        "    batch_variance = tf.reduce_mean(tf.square(x - batch_mean), axis=axes, keepdims=True)\n",
        "    x_norm = (x - batch_mean) / tf.sqrt(batch_variance + epsilon)\n",
        "    return gamma * x_norm + beta\n",
        "\n",
        "def custom_weight_norm(v, g, axis=None, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Custom Weight Normalization.\n",
        "    Reparameterizes a weight vector v using a scalar g such that:\n",
        "         w = (g / ||v||) * v,\n",
        "    where ||v|| is the Euclidean norm of v.\n",
        "\n",
        "    v: Weight vector (or tensor).\n",
        "    g: Learnable scalar (or tensor, broadcastable to v) controlling the magnitude.\n",
        "    axis: The axis (or axes) along which to compute the norm.\n",
        "    epsilon: Small constant for numerical stability.\n",
        "    \"\"\"\n",
        "    v_norm = tf.sqrt(tf.reduce_sum(tf.square(v), axis=axis, keepdims=True) + epsilon)\n",
        "    return (g / v_norm) * v\n",
        "\n",
        "def custom_layer_norm(x, gamma, beta, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Custom Layer Normalization.\n",
        "    Normalizes the input tensor x across the feature dimension for each sample.\n",
        "\n",
        "    x: Input tensor with shape [..., features].\n",
        "    gamma: Learnable scale parameter (broadcastable to x).\n",
        "    beta: Learnable shift parameter (broadcastable to x).\n",
        "    epsilon: Small constant for numerical stability.\n",
        "    \"\"\"\n",
        "    mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "    variance = tf.reduce_mean(tf.square(x - mean), axis=-1, keepdims=True)\n",
        "    x_norm = (x - mean) / tf.sqrt(variance + epsilon)\n",
        "    return gamma * x_norm + beta\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. CNN Model with Normalization Options\n",
        "# -----------------------------------------------------------\n",
        "class CustomCNN(tf.keras.Model):\n",
        "    def __init__(self, num_classes=10, norm_type='none'):\n",
        "        \"\"\"\n",
        "        A CNN model that applies the specified normalization in the forward pass.\n",
        "\n",
        "        norm_type: 'batch', 'layer', 'weight', or 'none'\n",
        "        \"\"\"\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.norm_type = norm_type\n",
        "\n",
        "        if self.norm_type != 'weight':\n",
        "            # Standard convolution layer if not using Weight Normalization.\n",
        "            self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', use_bias=True)\n",
        "        else:\n",
        "            # For Weight Normalization, we reparameterize the kernel manually.\n",
        "            self.kernel_shape = (3, 3, 1, 32)\n",
        "            self.v = tf.Variable(tf.random.normal(self.kernel_shape, stddev=0.1), trainable=True)\n",
        "            self.g = tf.Variable(tf.ones((1,)), trainable=True)\n",
        "            self.conv1_bias = tf.Variable(tf.zeros([32]), trainable=True)\n",
        "\n",
        "        # For Batch Normalization, create learnable gamma and beta parameters.\n",
        "        if self.norm_type == 'batch':\n",
        "            self.gamma_bn = tf.Variable(tf.ones([1, 1, 1, 32]), trainable=True)\n",
        "            self.beta_bn  = tf.Variable(tf.zeros([1, 1, 1, 32]), trainable=True)\n",
        "        # For Layer Normalization, create gamma and beta (per channel) parameters.\n",
        "        elif self.norm_type == 'layer':\n",
        "            self.gamma_ln = tf.Variable(tf.ones([32]), trainable=True)\n",
        "            self.beta_ln  = tf.Variable(tf.zeros([32]), trainable=True)\n",
        "\n",
        "        self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.fc = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # Convolution operation\n",
        "        if self.norm_type == 'weight':\n",
        "            # Use custom weight normalization for conv kernel.\n",
        "            kernel = custom_weight_norm(self.v, self.g, axis=[0,1,2])\n",
        "            conv = tf.nn.conv2d(x, kernel, strides=1, padding='SAME') + self.conv1_bias\n",
        "        else:\n",
        "            conv = self.conv1(x)\n",
        "\n",
        "        # Apply normalization based on the chosen norm_type.\n",
        "        if self.norm_type == 'batch':\n",
        "            conv = custom_batch_norm(conv, self.gamma_bn, self.beta_bn)\n",
        "        elif self.norm_type == 'layer':\n",
        "            # For LayerNorm, first reshape to combine spatial dimensions.\n",
        "            shape = tf.shape(conv)\n",
        "            conv_reshaped = tf.reshape(conv, [shape[0], -1, conv.shape[-1]])\n",
        "            conv_norm = custom_layer_norm(conv_reshaped, self.gamma_ln, self.beta_ln)\n",
        "            conv = tf.reshape(conv_norm, shape)\n",
        "        # If norm_type is 'none', do nothing extra.\n",
        "\n",
        "        # Activation and pooling\n",
        "        x_act = tf.nn.relu(conv)\n",
        "        x_pool = self.pool1(x_act)\n",
        "        x_flat = self.flatten(x_pool)\n",
        "        logits = self.fc(x_flat)\n",
        "        return logits\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3. Training Setup: Loss, Optimizer, and Training Step\n",
        "# -----------------------------------------------------------\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(images, training=True)\n",
        "        loss = loss_object(labels, logits)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 4. Main Function to Run Training, Evaluation, and Comparisons\n",
        "# -----------------------------------------------------------\n",
        "def main():\n",
        "    # Data Preparation: Load Fashion MNIST\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    x_train = np.expand_dims(x_train.astype(np.float32) / 255.0, -1)\n",
        "    x_test  = np.expand_dims(x_test.astype(np.float32) / 255.0, -1)\n",
        "\n",
        "    batch_size = 64\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "    num_epochs = 5\n",
        "\n",
        "    # Create models with different normalization options:\n",
        "    model_bn   = CustomCNN(norm_type='batch')\n",
        "    model_ln   = CustomCNN(norm_type='layer')\n",
        "    model_wn   = CustomCNN(norm_type='weight')\n",
        "    model_none = CustomCNN(norm_type='none')\n",
        "\n",
        "    # Train model with custom Batch Normalization.\n",
        "    print(\"Training model with custom Batch Normalization:\")\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "        for images, labels in train_dataset:\n",
        "            loss = train_step(model_bn, images, labels)\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "        print(f\"Epoch {epoch+1} (Batch Norm): Loss = {total_loss / steps:.4f}\")\n",
        "\n",
        "    # Train model with custom Layer Normalization.\n",
        "    print(\"\\nTraining model with custom Layer Normalization:\")\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "        for images, labels in train_dataset:\n",
        "            loss = train_step(model_ln, images, labels)\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "        print(f\"Epoch {epoch+1} (Layer Norm): Loss = {total_loss / steps:.4f}\")\n",
        "\n",
        "    # Train model with custom Weight Normalization.\n",
        "    print(\"\\nTraining model with custom Weight Normalization:\")\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "        for images, labels in train_dataset:\n",
        "            loss = train_step(model_wn, images, labels)\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "        print(f\"Epoch {epoch+1} (Weight Norm): Loss = {total_loss / steps:.4f}\")\n",
        "\n",
        "    # Train model with No Normalization.\n",
        "    print(\"\\nTraining model with No Normalization:\")\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "        for images, labels in train_dataset:\n",
        "            loss = train_step(model_none, images, labels)\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "        print(f\"Epoch {epoch+1} (No Norm): Loss = {total_loss / steps:.4f}\")\n",
        "\n",
        "    # Evaluate one of the models on Test Data (using custom Batch Norm model as an example)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    for images, labels in test_dataset:\n",
        "        logits = model_bn(images, training=False)\n",
        "        test_accuracy.update_state(labels, logits)\n",
        "    print(f\"\\nTest Accuracy (Custom Batch Norm): {test_accuracy.result().numpy() * 100:.2f}%\")\n",
        "\n",
        "    # -----------------------------------------------------------\n",
        "    # 5. Comparison with TensorFlow Built-In Normalization Functions\n",
        "    # Compare custom Batch Normalization with tf.keras.layers.BatchNormalization.\n",
        "    sample_input = tf.random.normal([32, 28, 28, 32])\n",
        "    custom_bn_output = custom_batch_norm(sample_input,\n",
        "                                         gamma=tf.ones([1,1,1,32]),\n",
        "                                         beta=tf.zeros([1,1,1,32]))\n",
        "    bn_layer = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-5)\n",
        "    tf_bn_output = bn_layer(sample_input, training=True)\n",
        "    difference = tf.reduce_mean(tf.abs(custom_bn_output - tf_bn_output))\n",
        "    print(f\"\\nMean absolute difference between custom BN and TF BN: {difference.numpy():.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}